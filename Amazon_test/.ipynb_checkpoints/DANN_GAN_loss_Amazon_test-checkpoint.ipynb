{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.insert(0, \"/home/jay/DSN/tensorflow/ADSA/Model/\")\n",
    "import cnnet as nn\n",
    "import util\n",
    "import read_data as read_data\n",
    "import ADSA_DNN as ADSA\n",
    "import DataPackage as dp\n",
    "import AmazonReviewsFeaturePlot as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "option='Amazon'\n",
    "source_data, target_data, max_feature = read_data.read(option)\n",
    "train_ftd_source, train_labeld_source = source_data[0]\n",
    "valid_ftd_source, valid_labeld_source = source_data[1]\n",
    "test_ftd_source, test_labeld_source = source_data[2]\n",
    "    \n",
    "train_ftd_target, train_labeld_target = target_data[0]\n",
    "valid_ftd_target, valid_labeld_target = target_data[1]\n",
    "test_ftd_target, test_labeld_target = target_data[2]\n",
    "\n",
    "batch_size=128\n",
    "x_dim=max_feature\n",
    "y_dim=2\n",
    "# Model construction\n",
    "struct=ADSA.ADSA_struct()\n",
    "struct.FE_struct.layer_struct=[max_feature,400,100]\n",
    "struct.FE_struct.activation=[tf.nn.relu, tf.nn.relu]\n",
    "struct.PSFE_struct.layer_struct=[max_feature,400,100]\n",
    "struct.PSFE_struct.activation=[tf.nn.relu, tf.nn.relu]\n",
    "struct.PTFE_struct.layer_struct=[max_feature,400,100]\n",
    "struct.PTFE_struct.activation=[tf.nn.relu, tf.nn.relu]\n",
    "struct.TC_struct.layer_struct=[100,50, 2]\n",
    "struct.TC_struct.activation=[tf.nn.relu, 'linear']\n",
    "struct.DC_struct.layer_struct=[100, 50, 2]\n",
    "struct.DC_struct.activation=[tf.nn.relu, 'linear']\n",
    "struct.Sep_struct.layer_struct=[100, 50, 3]\n",
    "struct.Sep_struct.activation=[tf.nn.relu, 'linear']\n",
    "struct.batch_size=batch_size\n",
    "\n",
    "description='Amazon_ADSA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared_Feature_extractor is constructed with hidden layer number 1\n",
      "Source_private_feature_extractor is constructed with hidden layer number 1\n",
      "Target_private_feature_extractor is constructed with hidden layer number 1\n",
      "Task_classifier is constructed with hidden layer number 1\n",
      "Domain_adapt_classifier is constructed with hidden layer number 1\n",
      "Domain_sep_classifier is constructed with hidden layer number 1\n"
     ]
    }
   ],
   "source": [
    "# Build the model graph\n",
    "graph=tf.get_default_graph()\n",
    "with graph.as_default():\n",
    "    model = ADSA.ADSAModel(struct, x_dim, y_dim)\n",
    "    learning_rate= tf.placeholder(tf.float32, [])\n",
    "    \n",
    "    pred_loss = tf.reduce_mean(model.pred_loss)\n",
    "    domain_loss = tf.reduce_mean(model.domain_adapt_loss)\n",
    "    sep_loss=tf.reduce_mean(model.domain_sep_loss)\n",
    "    total_loss = pred_loss - domain_loss+0.5*sep_loss\n",
    "    # optimizer\n",
    "    dann_train_op = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(total_loss)\n",
    "    \n",
    "    domain_adapt_classifier_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'domain_predict')\n",
    "    adapt_opt=tf.train.MomentumOptimizer(learning_rate, 0.009).minimize(domain_loss, var_list=domain_adapt_classifier_vars)\n",
    "    \n",
    "    target_FE_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'shared_feature_extractor')\n",
    "    gen_opt=tf.train.MomentumOptimizer(learning_rate,0.009).minimize(-domain_loss, var_list=target_FE_vars)\n",
    "    \n",
    "    domain_sep_classifier_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'domain_sep_classifier')\n",
    "    sep_opt=tf.train.MomentumOptimizer(learning_rate,0.009).minimize(sep_loss,var_list=domain_sep_classifier_vars)\n",
    "    \n",
    "    source_private_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'source_private_feature_extractor')\n",
    "    target_private_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'target_private_feature_extractor')\n",
    "    sep_encoder_vars=target_FE_vars+source_private_vars+target_private_vars\n",
    "    sep_gen_opt=tf.train.MomentumOptimizer(learning_rate,0.009).minimize(sep_loss,var_list=sep_encoder_vars)\n",
    "    \n",
    "    # Accuracy evaluation\n",
    "    correct_label_pred = tf.equal(tf.argmax(model.classify_labels, 1), tf.argmax(model.pred, 1))\n",
    "    label_acc = tf.reduce_mean(tf.cast(correct_label_pred, tf.float32))\n",
    "    correct_domain_pred = tf.equal(tf.argmax(model.domain_labels, 1), tf.argmax(model.domain_adapt_pred, 1))\n",
    "    domain_acc = tf.reduce_mean(tf.cast(correct_domain_pred, tf.float32))\n",
    "    correct_sep_pred=tf.equal(tf.argmax(model.domain_sep_labels, 1), tf.argmax(model.domain_sep_pred, 1))\n",
    "    sep_acc=tf.reduce_mean(tf.cast(correct_sep_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epoch=2000\n",
    "def train_and_evaluate(training_mode, graph, model, verbose=False):\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # Batch generators\n",
    "        t_source_batch=util.batch_generator([train_ftd_source, train_labeld_source], batch_size/2)\n",
    "        t_target_batch=util.batch_generator([train_ftd_target, train_labeld_target], batch_size/2)\n",
    "        \n",
    "        # adaptation discriminator labels and separation discriminator labels\n",
    "        domain_labels=np.vstack([np.tile([1, 0], [batch_size/2, 1]), np.tile([0, 1],[batch_size/2, 1])])\n",
    "        domain_sep_labels=np.vstack([np.tile([1, 0, 0], [batch_size, 1]), np.tile([0, 1, 0],[batch_size/2, 1]), np.tile([0, 0, 1],[batch_size/2, 1])])\n",
    "        \n",
    "        for itr in range(num_epoch):\n",
    "            #learning rate\n",
    "            p = float(itr) / num_epoch\n",
    "            flip_scale= 2. / (1. + np.exp(-10. * p)) \n",
    "            lr=0.01 / (1. + 10 * p)**0.75\n",
    "            \n",
    "            if training_mode=='ADSA':\n",
    "                \n",
    "                d_source_batch=util.batch_generator([train_ftd_source, train_labeld_source], batch_size/2)\n",
    "                d_target_batch=util.batch_generator([train_ftd_target, train_labeld_target], batch_size/2)\n",
    "                gen_source_batch= util.batch_generator([train_ftd_source, train_labeld_source], batch_size / 2)\n",
    "                gen_target_batch= util.batch_generator([train_ftd_target, train_labeld_target], batch_size / 2)\n",
    "                #update the adaptation discriminator and separation discriminator\n",
    "                for d_iter in range(20):\n",
    "                    dX0, dy0= d_source_batch.next()\n",
    "                    dX1, dy1= d_target_batch.next()\n",
    "                    dX=np.vstack([dX0,dX1])\n",
    "                    dy=np.vstack([dy0, dy1])\n",
    "                \n",
    "                    _, d_loss=sess.run([adapt_opt, domain_loss], feed_dict={model.X: dX, model.y: dy,\n",
    "                                                                            model.domain_labels: domain_labels,\n",
    "                                                                            model.train_flag: True,\n",
    "                                                                            model.flip_scale: flip_scale,\n",
    "                                                                            learning_rate: lr})\n",
    "                    if d_iter %10 ==0:\n",
    "                        _, s_loss,s_acc=sess.run([sep_opt, sep_loss, sep_acc], feed_dict={model.X: dX, model.y: dy,\n",
    "                                                                            model.domain_labels: domain_labels,\n",
    "                                                                            model.domain_sep_labels: domain_sep_labels,\n",
    "                                                                            model.train_flag: True,\n",
    "                                                                            model.flip_scale: flip_scale,\n",
    "                                                                            learning_rate: lr})\n",
    "                # update shared encoder source individual encoder and target individual encoder\n",
    "                for g_iter in range(10):\n",
    "                    gX0, gy0 =gen_source_batch.next()\n",
    "                    gX1, gy1 = gen_target_batch.next()\n",
    "                    gX=np.vstack([gX0, gX1])\n",
    "                    gy=np.vstack([gy0, gy1])\n",
    "                    _, g_loss=sess.run([gen_opt, domain_loss],feed_dict={model.X: gX, model.y: gy,\n",
    "                                                                         model.domain_labels: domain_labels,\n",
    "                                                                            model.train_flag: True,\n",
    "                                                                            model.flip_scale: flip_scale,\n",
    "                                                                            learning_rate: lr})\n",
    "                    \n",
    "                    _, s_loss=sess.run([sep_gen_opt, sep_loss],feed_dict={model.X: gX, model.y: gy,\n",
    "                                                                          model.domain_sep_labels: domain_sep_labels,\n",
    "                                                                          model.train_flag: True,\n",
    "                                                                          model.flip_scale: flip_scale,\n",
    "                                                                          learning_rate: lr})\n",
    "                # update all \n",
    "                X0, y0 =t_source_batch.next()\n",
    "                X1, y1 = t_target_batch.next()\n",
    "                X=np.vstack([X0, X1])\n",
    "                y=np.vstack([y0, y1])\n",
    "                _, batch_loss, p_acc, d_acc=\\\n",
    "                        sess.run([dann_train_op, total_loss,label_acc, domain_acc], feed_dict={model.X: X,model.y:y,\n",
    "                                                                                               model.domain_labels:domain_labels,\n",
    "                                                                                               model.domain_sep_labels:domain_sep_labels,\n",
    "                                                                                               model.train_flag: True,\n",
    "                                                                                               model.flip_scale: flip_scale,\n",
    "                                                                                               learning_rate:lr})\n",
    "                \n",
    "                if verbose and itr % 100==0:\n",
    "                    print ('loss: %f adapt_acc: %f task_acc: %f sep acc: %f\\n'\n",
    "                           'flip_scale: %f  learning_rate: %f' %\n",
    "                           (batch_loss, d_acc, p_acc,s_acc, flip_scale, lr))\n",
    "        # test\n",
    "        source_acc = sess.run(label_acc,\n",
    "                            feed_dict={model.X: test_ftd_source, model.y: test_labeld_source,\n",
    "                                       model.train_flag: False})\n",
    "\n",
    "        target_acc = sess.run(label_acc,\n",
    "                            feed_dict={model.X: test_ftd_target, model.y: test_labeld_target,\n",
    "                                       model.train_flag: False})\n",
    "        \n",
    "        \n",
    "        \n",
    "    return source_acc, target_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain adaptation training\n",
      "loss: 0.547829 adapt_acc: 0.445312 task_acc: 0.453125 sep acc: 0.203125\n",
      "flip_scale: 1.000000  learning_rate: 0.010000\n",
      "loss: 0.025800 adapt_acc: 0.578125 task_acc: 0.406250 sep acc: 1.000000\n",
      "flip_scale: 1.244919  learning_rate: 0.007378\n",
      "loss: -0.030833 adapt_acc: 0.515625 task_acc: 0.703125 sep acc: 1.000000\n",
      "flip_scale: 1.462117  learning_rate: 0.005946\n",
      "loss: -0.060310 adapt_acc: 0.492188 task_acc: 0.718750 sep acc: 1.000000\n",
      "flip_scale: 1.635149  learning_rate: 0.005030\n",
      "loss: -0.112889 adapt_acc: 0.507812 task_acc: 0.734375 sep acc: 1.000000\n",
      "flip_scale: 1.761594  learning_rate: 0.004387\n",
      "loss: -0.249090 adapt_acc: 0.578125 task_acc: 0.828125 sep acc: 1.000000\n",
      "flip_scale: 1.848284  learning_rate: 0.003908\n",
      "loss: -0.339880 adapt_acc: 0.515625 task_acc: 0.875000 sep acc: 1.000000\n",
      "flip_scale: 1.905148  learning_rate: 0.003536\n",
      "loss: -0.360324 adapt_acc: 0.507812 task_acc: 0.875000 sep acc: 1.000000\n",
      "flip_scale: 1.941376  learning_rate: 0.003237\n",
      "loss: -0.430987 adapt_acc: 0.562500 task_acc: 0.890625 sep acc: 1.000000\n",
      "flip_scale: 1.964028  learning_rate: 0.002991\n",
      "loss: -0.472961 adapt_acc: 0.468750 task_acc: 0.906250 sep acc: 1.000000\n",
      "flip_scale: 1.978026  learning_rate: 0.002784\n",
      "loss: -0.528642 adapt_acc: 0.421875 task_acc: 0.968750 sep acc: 1.000000\n",
      "flip_scale: 1.986614  learning_rate: 0.002608\n",
      "loss: -0.561388 adapt_acc: 0.523438 task_acc: 0.968750 sep acc: 1.000000\n",
      "flip_scale: 1.991860  learning_rate: 0.002456\n",
      "loss: -0.624200 adapt_acc: 0.484375 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.995055  learning_rate: 0.002324\n",
      "loss: -0.637579 adapt_acc: 0.484375 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.996998  learning_rate: 0.002207\n",
      "loss: -0.650384 adapt_acc: 0.484375 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.998178  learning_rate: 0.002102\n",
      "loss: -0.655294 adapt_acc: 0.468750 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.998894  learning_rate: 0.002009\n",
      "loss: -0.653037 adapt_acc: 0.523438 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.999329  learning_rate: 0.001925\n",
      "loss: -0.663769 adapt_acc: 0.437500 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.999593  learning_rate: 0.001848\n",
      "loss: -0.665623 adapt_acc: 0.476562 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.999753  learning_rate: 0.001778\n",
      "loss: -0.664786 adapt_acc: 0.484375 task_acc: 1.000000 sep acc: 1.000000\n",
      "flip_scale: 1.999850  learning_rate: 0.001714\n",
      "Source (MNIST) accuracy: 0.785890\n",
      "Target (MNIST-M) accuracy: 0.757948\n"
     ]
    }
   ],
   "source": [
    "print ('\\nDomain adaptation training')\n",
    "source_acc, target_acc= train_and_evaluate('ADSA', graph, model,True)\n",
    "print ('Source (MNIST) accuracy: %f'% source_acc)\n",
    "print ('Target (MNIST-M) accuracy: %f'% target_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
