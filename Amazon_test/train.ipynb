{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.insert(0, \"/home/jay/DSN/tensorflow/ADSA/Model/\")\n",
    "import cnnet as nn\n",
    "import util\n",
    "import read_data as read_data\n",
    "import ADSA_DNN_maxmin as ADSA\n",
    "import DataPackage as dp\n",
    "import AmazonReviewsFeaturePlot as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "option='Amazon'\n",
    "source_data, target_data, max_feature = read_data.read(option)\n",
    "train_ftd_source, train_labeld_source = source_data[0]\n",
    "valid_ftd_source, valid_labeld_source = source_data[1]\n",
    "test_ftd_source, test_labeld_source = source_data[2]\n",
    "    \n",
    "train_ftd_target, train_labeld_target = target_data[0]\n",
    "valid_ftd_target, valid_labeld_target = target_data[1]\n",
    "test_ftd_target, test_labeld_target = target_data[2]\n",
    "\n",
    "batch_size=[80,80]\n",
    "x_dim=max_feature\n",
    "y_dim=2\n",
    "struct=ADSA.ADSA_struct()\n",
    "struct.FE_struct.layer_struct=[max_feature,500,100]\n",
    "struct.FE_struct.activation=[tf.nn.relu, tf.nn.relu]\n",
    "#struct.FE_struct.activation=[tf.nn.sigmoid, tf.nn.sigmoid]\n",
    "struct.FE_struct.batch_norm=[True, True]\n",
    "struct.PSFE_struct.layer_struct=[max_feature,500,100]\n",
    "struct.PSFE_struct.activation=[tf.nn.relu, tf.nn.relu]\n",
    "#struct.PSFE_struct.activation=[tf.nn.sigmoid, tf.nn.sigmoid]\n",
    "struct.PSFE_struct.batch_norm=[True,True]\n",
    "struct.PTFE_struct.layer_struct=[max_feature,500,100]\n",
    "struct.PTFE_struct.activation=[tf.nn.relu, tf.nn.relu]\n",
    "#struct.PTFE_struct.activation=[tf.nn.sigmoid, tf.nn.sigmoid]\n",
    "struct.PTFE_struct.batch_norm=[True,True]\n",
    "struct.TC_struct.layer_struct=[100, 2]\n",
    "struct.TC_struct.activation=['linear']\n",
    "struct.TC_struct.batch_norm=[False]\n",
    "struct.DC_struct.layer_struct=[100,10, 2]\n",
    "struct.DC_struct.activation=[ tf.nn.relu, 'linear']\n",
    "struct.DC_struct.batch_norm=[False,False]\n",
    "struct.Sep_struct.layer_struct=[100, 3]\n",
    "struct.Sep_struct.activation=['linear']\n",
    "struct.Sep_struct.batch_norm=[False]\n",
    "\n",
    "description='Amazon_DSN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared_Feature_extractor is constructed with hidden layer number 1\n",
      "Source_private_feature_extractor is constructed with hidden layer number 1\n",
      "Target_private_feature_extractor is constructed with hidden layer number 1\n",
      "Task_classifier is constructed with hidden layer number 0\n",
      "Domain_adapt_classifier is constructed with hidden layer number 1\n",
      "Domain_sep_classifier is constructed with hidden layer number 0\n"
     ]
    }
   ],
   "source": [
    "# Build the model graph\n",
    "graph=tf.get_default_graph()\n",
    "with graph.as_default():\n",
    "    model = ADSA.ADSAModel(struct,x_dim, y_dim)\n",
    "    learning_rate= tf.placeholder(tf.float32, [])\n",
    "    \n",
    "    pred_loss = tf.reduce_mean(model.pred_loss)\n",
    "    domain_loss = tf.reduce_mean(model.domain_adapt_loss)\n",
    "    sep_loss=tf.reduce_mean(model.domain_sep_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    domain_adapt_classifier_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'domain_predict')\n",
    "    adapt_opt=tf.train.MomentumOptimizer(learning_rate, 0.09).minimize(domain_loss, var_list=domain_adapt_classifier_vars)\n",
    "    #adapt_opt=tf.train.AdamOptimizer(learning_rate=0.00005).minimize(domain_loss, var_list=domain_adapt_classifier_vars)\n",
    "                            \n",
    "    target_FE_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'shared_feature_extractor')\n",
    "    gen_opt=tf.train.MomentumOptimizer(learning_rate,0.09).minimize(-domain_loss, var_list=target_FE_vars)\n",
    "    #gen_opt=tf.train.AdamOptimizer(learning_rate=0.00005).minimize(-domain_loss, var_list=target_FE_vars)\n",
    "                            \n",
    "    domain_sep_classifier_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'domain_sep_classifier')\n",
    "    sep_opt=tf.train.MomentumOptimizer(learning_rate,0.09).minimize(-sep_loss,var_list=domain_sep_classifier_vars)\n",
    "    #sep_opt=tf.train.AdamOptimizer(learning_rate=0.00005).minimize(-sep_loss,var_list=domain_sep_classifier_vars)\n",
    "                            \n",
    "    source_private_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'source_private_feature_extractor')\n",
    "    target_private_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'target_private_feature_extractor')\n",
    "    sep_encoder_vars=target_FE_vars+source_private_vars+target_private_vars\n",
    "    sep_gen_opt=tf.train.MomentumOptimizer(learning_rate,0.9).minimize(sep_loss,var_list=sep_encoder_vars)\n",
    "    #sep_gen_opt=tf.train.AdamOptimizer(learning_rate=0.00005).minimize(sep_loss,var_list=sep_encoder_vars)\n",
    "    \n",
    "    task_d_vars=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'label_predictor')\n",
    "    pre_train_op=tf.train.AdamOptimizer(learning_rate=0.0005).minimize(pred_loss, var_list=target_FE_vars+task_d_vars)\n",
    "    \n",
    "    total_loss = pred_loss - domain_loss+0.05*sep_loss+tf.reduce_mean(tf.nn.l2_normalize(task_d_vars[0],1))\n",
    "    dann_train_op = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(total_loss)\n",
    "    #dann_train_op = tf.train.AdamOptimizer(learning_rate=0.00005).minimize(total_loss)\n",
    "    \n",
    "    # Evaluation\n",
    "    source_correct_label_pred = tf.equal(tf.argmax(model.source_label, 1), tf.argmax(model.source_pred, 1))\n",
    "    source_acc = tf.reduce_mean(tf.cast(source_correct_label_pred, tf.float32))\n",
    "    \n",
    "    target_correct_label_pred = tf.equal(tf.argmax(model.target_label, 1), tf.argmax(model.target_pred, 1))\n",
    "    target_acc = tf.reduce_mean(tf.cast(target_correct_label_pred, tf.float32))\n",
    "    \n",
    "    correct_domain_pred = tf.equal(tf.argmax(model.domain_labels, 1), tf.argmax(model.domain_adapt_pred, 1))\n",
    "    domain_acc = tf.reduce_mean(tf.cast(correct_domain_pred, tf.float32))\n",
    "                            \n",
    "    correct_sep_pred=tf.equal(tf.argmax(model.domain_sep_labels, 1), tf.argmax(model.domain_sep_pred, 1))\n",
    "    sep_acc=tf.reduce_mean(tf.cast(correct_sep_pred, tf.float32))\n",
    "    \n",
    "    #get feauture\n",
    "    latent_feats=model.get_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epoch=2000\n",
    "def train_and_evaluate(training_mode, graph, model, verbose=False):\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        p_source_batch=util.batch_generator([train_ftd_source, train_labeld_source], batch_size[0])\n",
    "        p_target_batch=util.batch_generator([train_ftd_target, train_labeld_target], batch_size[1])\n",
    "        for i in range(1000):\n",
    "            X0, y0= p_source_batch.next()\n",
    "            X1, y1= p_target_batch.next()\n",
    "            X=np.vstack([X0,X1])\n",
    "            y=np.vstack([y0,y1])\n",
    "            _, batch_loss, s_acc=\\\n",
    "                        sess.run([pre_train_op, pred_loss, source_acc], feed_dict={model.X: X,model.y:y,\n",
    "                                                                        model.batch_size: batch_size,\n",
    "                                                                        model.train_flag: False,\n",
    "                                                                        model.flip_scale: 0,\n",
    "                                                                        learning_rate:0.05})\n",
    "            #print('    pre loss:%f,  source acc %f'%(batch_loss,np.mean(s_acc)))\n",
    "        \n",
    "        # Batch generators\n",
    "        t_source_batch=util.batch_generator([train_ftd_source, train_labeld_source], batch_size[0])\n",
    "        t_target_batch=util.batch_generator([train_ftd_target, train_labeld_target], batch_size[1])\n",
    "        \n",
    "        domain_labels=np.vstack([np.tile([0.9, 0.1], [batch_size[0], 1]), np.tile([0.1, 0.9],[batch_size[1], 1])])\n",
    "        domain_sep_labels=np.vstack([np.tile([0.8, 0.1, 0.1], [batch_size[0]+batch_size[1], 1]), np.tile([0.1, 0.8, 0.1],[batch_size[0], 1]), np.tile([0.1, 0.1, 0.8],[batch_size[1], 1])])\n",
    "        \n",
    "        for itr in range(num_epoch):\n",
    "            p = float(itr) / num_epoch\n",
    "            flip_scale= 2. / (1. + np.exp(-10. * p)) \n",
    "            #lr=0.01 / (1. + 10 * p)**0.75\n",
    "            lr=0.1/(1.+10*p)**0.75\n",
    "            if training_mode=='dann':\n",
    "                \n",
    "                d_source_batch=util.batch_generator([train_ftd_source, train_labeld_source], batch_size[0])\n",
    "                d_target_batch=util.batch_generator([train_ftd_target, train_labeld_target], batch_size[1])\n",
    "                gen_source_batch= util.batch_generator([train_ftd_source, train_labeld_source], batch_size[0])\n",
    "                gen_target_batch= util.batch_generator([train_ftd_target, train_labeld_target], batch_size[1])\n",
    "                sep_source_batch= util.batch_generator([train_ftd_source, train_labeld_source], batch_size[0])\n",
    "                sep_target_batch= util.batch_generator([train_ftd_target, train_labeld_target], batch_size[1])\n",
    "                \n",
    "                for d_iter in range(40):\n",
    "                    dX0, dy0= d_source_batch.next()\n",
    "                    dX1, dy1= d_target_batch.next()\n",
    "                    dX=np.vstack([dX0,dX1])\n",
    "                    dy=np.vstack([dy0, dy1])\n",
    "                \n",
    "                    _, d_loss=sess.run([adapt_opt, domain_loss], feed_dict={model.X: dX, model.y: dy,\n",
    "                                                                            model.domain_labels: domain_labels,\n",
    "                                                                            model.batch_size: batch_size,\n",
    "                                                                            model.train_flag: True,\n",
    "                                                                            model.flip_scale: flip_scale,\n",
    "                                                                            learning_rate: lr})\n",
    "                   \n",
    "                for d_iter in range(2):\n",
    "                    sX0, sy0 =sep_source_batch.next()\n",
    "                    sX1, sy1 = sep_target_batch.next()\n",
    "                    sX=np.vstack([sX0, sX1])\n",
    "                    sy=np.vstack([sy0, sy1])\n",
    "                    _, s_loss=sess.run([sep_opt, sep_loss], feed_dict={model.X: sX, model.y: sy,\n",
    "                                                                       model.domain_labels: domain_labels,\n",
    "                                                                       model.domain_sep_labels: domain_sep_labels,\n",
    "                                                                       model.batch_size: batch_size,\n",
    "                                                                       model.train_flag: True,\n",
    "                                                                       model.flip_scale: flip_scale,\n",
    "                                                                       learning_rate: 0.005})\n",
    "                \n",
    "                #print('   train D domain loss : %f\\n'% d_loss)\n",
    "                for g_iter in range(4):\n",
    "                    gX0, gy0 =gen_source_batch.next()\n",
    "                    gX1, gy1 = gen_target_batch.next()\n",
    "                    gX=np.vstack([gX0, gX1])\n",
    "                    gy=np.vstack([gy0, gy1])\n",
    "                    _, g_loss=sess.run([gen_opt, domain_loss],feed_dict={model.X: gX, model.y: gy,\n",
    "                                                                         model.domain_labels: domain_labels,\n",
    "                                                                         model.batch_size: batch_size,\n",
    "                                                                         model.train_flag: True,\n",
    "                                                                         model.flip_scale: flip_scale,\n",
    "                                                                         learning_rate: lr})\n",
    "                \n",
    "                for g_iter in range(2):\n",
    "                    sX0, sy0 =sep_source_batch.next()\n",
    "                    sX1, sy1 = sep_target_batch.next()\n",
    "                    sX=np.vstack([sX0, sX1])\n",
    "                    sy=np.vstack([sy0, sy1])\n",
    "                    _, s_loss=sess.run([sep_gen_opt, sep_loss],feed_dict={model.X: sX, model.y: sy,\n",
    "                                                                          model.domain_sep_labels: domain_sep_labels,\n",
    "                                                                          model.batch_size: batch_size,\n",
    "                                                                          model.train_flag: True,\n",
    "                                                                          model.flip_scale: flip_scale,\n",
    "                                                                          learning_rate: lr})\n",
    "                   \n",
    "                #print('   train G domain loss : %f\\n'% g_loss)\n",
    "                X0, y0 =t_source_batch.next()\n",
    "                X1, y1 = t_target_batch.next()\n",
    "                X=np.vstack([X0, X1])\n",
    "                y=np.vstack([y0, y1])\n",
    "                _, batch_loss, p_acc, d_acc ,s_acc=\\\n",
    "                        sess.run([dann_train_op, total_loss,source_acc, domain_acc,sep_acc], feed_dict={model.X: X,model.y:y,\n",
    "                                                                                               model.domain_labels:domain_labels,\n",
    "                                                                                               model.domain_sep_labels: domain_sep_labels,\n",
    "                                                                                               model.batch_size: batch_size,\n",
    "                                                                                               model.train_flag: True,\n",
    "                                                                                               model.flip_scale: flip_scale,\n",
    "                                                                                               learning_rate:lr})\n",
    "                \n",
    "                if verbose and itr % 100==0:\n",
    "                    print ('epoch %i  loss: %f adapt_acc: %f task_acc: %f  sep acc: %f \\n'\n",
    "                           '          flip_scale: %f  learning_rate: %f' %\n",
    "                           (itr,batch_loss, d_acc, p_acc, s_acc, flip_scale, lr))\n",
    "                    \n",
    "                if verbose and itr%1000==0:\n",
    "                    val_size=[valid_ftd_source.shape[0],valid_ftd_target.shape[0]]\n",
    "                    val_data=np.vstack([valid_ftd_source, valid_ftd_target])\n",
    "                    val_label=np.vstack([valid_labeld_source, valid_labeld_target])\n",
    "                    val_s_acc, val_t_acc=\\\n",
    "                        sess.run([source_acc, target_acc], feed_dict={model.X: val_data,model.y:val_label,\n",
    "                                                                     model.batch_size: val_size,\n",
    "                                                                     model.train_flag: False,\n",
    "                                                                    })\n",
    "                    print('      val source acc: %f  target acc: %f \\n'% (val_s_acc, val_t_acc))\n",
    "                \n",
    "        # test\n",
    "        test_size=[test_ftd_source.shape[0],test_ftd_target.shape[0]]\n",
    "        test_data=np.vstack([test_ftd_source,test_ftd_target])\n",
    "        test_label=np.vstack([test_labeld_source, test_labeld_target])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        source_ac, target_ac,l_f= sess.run([source_acc, target_acc,latent_feats],\n",
    "                            feed_dict={model.X: test_data, model.y: test_label,\n",
    "                                       model.batch_size: test_size, model.train_flag: False})\n",
    "        \n",
    "       \n",
    "        \n",
    "    return source_ac*100, target_ac*100, l_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain adaptation training\n",
      "epoch 0  loss: -0.541673 adapt_acc: 0.843750 task_acc: 1.000000  sep acc: 0.409375 \n",
      "          flip_scale: 1.000000  learning_rate: 0.100000\n",
      "      val source acc: 0.870000  target acc: 0.710000 \n",
      "\n",
      "epoch 100  loss: -1.309022 adapt_acc: 0.631250 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.244919  learning_rate: 0.073779\n",
      "epoch 200  loss: -1.360310 adapt_acc: 0.518750 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.462117  learning_rate: 0.059460\n",
      "epoch 300  loss: -1.317948 adapt_acc: 0.568750 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.635149  learning_rate: 0.050297\n",
      "epoch 400  loss: -1.315751 adapt_acc: 0.631250 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.761594  learning_rate: 0.043869\n",
      "epoch 500  loss: -1.327483 adapt_acc: 0.575000 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.848284  learning_rate: 0.039079\n",
      "epoch 600  loss: -1.345623 adapt_acc: 0.568750 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.905148  learning_rate: 0.035355\n",
      "epoch 700  loss: -1.371187 adapt_acc: 0.487500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.941376  learning_rate: 0.032366\n",
      "epoch 800  loss: -1.355751 adapt_acc: 0.587500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.964028  learning_rate: 0.029907\n",
      "epoch 900  loss: -1.334132 adapt_acc: 0.637500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.978026  learning_rate: 0.027844\n",
      "epoch 1000  loss: -1.353316 adapt_acc: 0.543750 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.986614  learning_rate: 0.026085\n",
      "      val source acc: 0.800000  target acc: 0.660000 \n",
      "\n",
      "epoch 1100  loss: -1.351897 adapt_acc: 0.606250 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.991860  learning_rate: 0.024565\n",
      "epoch 1200  loss: -1.347401 adapt_acc: 0.612500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.995055  learning_rate: 0.023237\n",
      "epoch 1300  loss: -1.350080 adapt_acc: 0.587500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.996998  learning_rate: 0.022065\n",
      "epoch 1400  loss: -1.344058 adapt_acc: 0.637500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.998178  learning_rate: 0.021022\n",
      "epoch 1500  loss: -1.350146 adapt_acc: 0.575000 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.998894  learning_rate: 0.020088\n",
      "epoch 1600  loss: -1.354880 adapt_acc: 0.575000 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.999329  learning_rate: 0.019245\n",
      "epoch 1700  loss: -1.355323 adapt_acc: 0.500000 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.999593  learning_rate: 0.018480\n",
      "epoch 1800  loss: -1.361721 adapt_acc: 0.506250 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.999753  learning_rate: 0.017783\n",
      "epoch 1900  loss: -1.359717 adapt_acc: 0.537500 task_acc: 1.000000  sep acc: 1.000000 \n",
      "          flip_scale: 1.999850  learning_rate: 0.017144\n",
      "Source  accuracy: 86.560118\n",
      "Target  accuracy: 72.318041\n"
     ]
    }
   ],
   "source": [
    "print ('\\nDomain adaptation training')\n",
    "source_acc, target_acc, feats= train_and_evaluate('dann', graph, model,True)\n",
    "print ('Source  accuracy: %f'% source_acc)\n",
    "print ('Target  accuracy: %f'% target_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_n=2000\n",
    "#fp.features_plot(feats,source_data, target_data,sample_n, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
